{"./":{"url":"./","title":"Introduction","keywords":"","body":"示例 英文地址: https://scikit-learn.org/stable/auto_examples/index.html 其他示例 scikit-learn 的 Miscellaneous 和入门示例。 紧凑的估计表示 带有可视化API的ROC曲线 序回归 先进的绘图具有部分依赖 使用多输出估计器完成人脸 多标签分类 比较异常检测算法以对玩具数据集进行异常检测 具有随机投影嵌入的Johnson-Lindenstrauss边界 内核岭回归和SVR的比较 RBF内核的显式特征图逼近 双聚类 有关sklearn.cluster.bicluster模块的示例。 频谱共聚算法演示 频谱双聚类算法的演示 使用频谱共聚算法对文档进行聚合 校准 举例说明了对分类器的预测概率进行校准的示例。 分类器校准的比较 概率校准曲线 分类器的概率校准 3级分类的概率校准 分类 有关分类算法的一般示例。 分类法线和收缩线线性判别分析 识别手写数字 情节分类概率 分类器比较 线性和二次判别分析与协方差椭球 多聚类 有关sklearn.cluster模块的示例。 绘制层次聚类树状图 功能集聚 均值漂移聚类算法的演示 的k均值假设示范 在线学习面部表情字典 矢量量化示例 相似性传播聚类算法演示 有和没有结构的聚集聚类 分割区域中希腊硬币的图片 二维数字嵌入中的各种聚集聚类 K-means聚类 光谱聚类用于图像分割 硬币图像上的结构化Ward层次聚类演示 DBSCAN聚类算法演示 使用K均值的颜色量化 分层聚类：结构化与非结构化病房 具有不同指标的聚集集群 归纳聚类 OPTICS聚类算法演示 比较桦木和MiniBatchKMeans k均值初始化影响的实证评估 集群绩效评估中机会的调整 K-Means和MiniBatchKMeans聚类算法的比较 特征集聚与单变量选择 手写数字数据上的K-Means聚类演示 比较玩具数据集上的不同层次链接方法 在KMeans聚类上通过轮廓分析选择聚类数量 比较玩具数据集上的不同聚类算法 协方差估计 有关sklearn.covariance模块的示例。 Ledoit-Wolf与OAS估计 稀疏逆协方差估计 收缩协方差估计：LedoitWolf与OAS和最大似然性 健壮的协方差估计和马氏距离相关性 乐百氏VS实证协方差估计 交叉分解 有关sklearn.cross_decomposition模块的示例。 比较交叉分解方法 数据集示例 有关sklearn.datasets模块的示例。 Digit数据集 虹膜数据集 绘制随机生成的分类数据集 绘制随机生成的多标签数据集 决策树 有关sklearn.tree模块的示例。 决策树回归 多路输出决策树回归 在虹膜数据集上绘制决策树的决策面 使用成本复杂度修剪来修剪修剪决策树 了解决策树结构 分解 有关sklearn.decomposition模块的示例。 β-发散损失函数 具有虹膜数据集的PCA示例 增量 Iris数据集的LDA和PCA二维投影的比较 使用FastICA进行盲源分离 主成分分析（PCA） 2D点云上的 内核 概率PCA和因子分析（FA）进行模型选择 使用预先计算的字典进行稀疏编码 图片使用字典学习去噪 Faces数据集分解 集成方法 有关sklearn.ensemble模块的示例。 并行树木森林的像素重要性 使用AdaBoost进行决策树回归 绘制个人和投票回归预测 树木森林的功能重要性 IsolationForest示例 绘制VotingClassifier的决策边界 比较随机森林和多输出元估计器 梯度提升回归的预测间隔 梯度提升正则化 绘制由VotingClassifier计算的类概率 梯度推进回归 随机森林的OOB错误 两个级的AdaBoost 使用完全随机树的哈希特征转换 多类AdaBoosted决策树 离散相对真正的AdaBoost 使用堆叠组合预测 提前终止的梯度推进 带有树群的特征变换 梯度提升袋外估计 单一估计器与装袋：偏差方差分解 在虹膜数据集上绘制树木合奏的决策面 基于真实数据集的示例 具有一些中等大小的数据集或交互式用户界面的现实问题的应用程序。 真实数据集的异常值检测 压缩感测：使用L1先验（Lasso）进行层析成像重建 非负矩阵分解和隐含狄利克雷分布话题提取 使用特征脸和支持向量机的识别示例 模型复杂度影响 可视化的股市结构 维基百科的主要特征向量 物种分布建模 Libsvm 预测延迟 文本文档的核心分类 特征选择 有关sklearn.feature_selection模块的示例。 递归特征消除 F检验和相互信息的比较 管道Anova 通过交叉验证消除递归特征 使用SelectFromModel和LassoCV特征选择 与排列测试的分类评分的意义 单变量特征选择 高斯混合模型 有关sklearn.mixture模块的示例。 高斯混合的密度估计 高斯混合模型椭球 高斯混合模型选择 GMM协方差 高斯混合模型正弦曲线 贝叶斯高斯混合变量的浓度先验类型分析 高斯机器学习过程 有关sklearn.gaussian_process模块的示例。 XOR数据集上的高斯过程分类（GPC）的图示 虹膜数据集上的高斯过程分类（GPC） 核岭和高斯过程回归的比较 不同内核的先验和后验高斯过程的图示 高斯过程分类（GPC）的等概率线 概率预测的结果与高斯过程分类（GPC） 具有噪声水平估计的高斯过程回归（GPR） 高斯过程回归：基本入门示例 基于Mauna Loa CO2数据的高斯过程回归（GPR）。 离散数据结构上的高斯过程 广义线性模型 有关sklearn.linear_model模块的示例。 使用LARS的套索路径 绘制岭系数作为正则化的函数 SGD：最大余量分隔超平面 SGD：凸损失函数 普通最小二乘法和岭回归方差 绘制Ridge系数作为L2正则化的函数 SGD：罚款 多项式插值 物流功能 L1-Logistic回归的正规化道路 Logistic回归3类分类器 SGD：加权样本 线性回归示例 使用RANSAC进行稳健的线性模型估计 稀疏实施例：装修仅设有1和2 HuberRegressor VS岭集具有较强的异常 套索上密集和稀疏数据 比较各种在线求解器 多任务套索的联合特征选择 使用多项式逻辑+ L1的MNIST分类 在虹膜数据集上绘制多类 正交匹配追踪 套索和弹性网用于稀疏信号 贝叶斯岭回归的曲线拟合 Theil-Sen回归 绘制多项式和一对一静态Logistic回归 稳健的线性估计器拟合 Logistic回归中的L1惩罚和稀疏性 套索和弹性网络 自动相关性确定回归（ARD） 贝叶斯岭回归 20newgroups上的多类稀疏逻辑回归 套索模型选择：交叉验证/ AIC / 早期停止随机梯度下降的 检查 与sklearn.inspection模块有关的示例。 具有多重共线性或相关特征的置换重要性 排列重要性与随机森林特征重要性（MDI） 部分依赖图 流形学习 有关sklearn.manifold模块的示例。 使用LLE减少瑞士卷 流形学习方法的比较 多维缩放 叔SNE：各种困惑值对形状的影响 截断球面上的流形学习方法 手写数字流形学习：局部线性嵌入，Isomap… 缺失值插补 有关sklearn.impute模块的示例。 使用IterativeImputer的变体估算缺失值 在构建估算器之前估算缺失值 选型 与sklearn.model_selection模块有关的示例。 绘制交叉验证的预测 混淆矩阵 绘图验证曲线 拟合不足与拟合过度 使用带有交叉验证的网格搜索进行参数估计 对于比较估计超参数随机搜索和网格搜索 训练错误与测试错误 具有交叉验证的接收器操作特性（ROC） 嵌套与非嵌套交叉验证 在cross_val_score和GridSearchCV上进行多指标评估的演示 用于文本特征提取和评估的示例管道 平衡模型的复杂性和交叉验证的分数 在scikit-learn中可视化交叉验证行为 接收器工作特性（ROC） 精密召回 绘制学习曲线 多输出方法 有关sklearn.multioutput模块的示例。 分类器链 最近邻 有关sklearn.neighbors模块的示例。 最近邻居回归 使用局部离群因子（LOF）进行离群检测 最近邻居分类 最近质心分类 核密度估计 缓存最近的邻居 邻域成分分析图 具有局部异常值（LOF）的新颖性检测 比较具有和不具有邻域分量分析的最近邻域 使用邻域分量分析进行维 物种分布的核密度估计 简单的1D内核密度估计 TSNE中的近似最近邻居 神经网络 有关sklearn.neural_network模块的示例。 在MNIST上可视化MLP权重 用于数字分类的受限玻尔兹曼机功能 在多层感知变化正规化 比较随机学习策略MLPClassifier 管道和复合估计器 由其他估算器组成变压器和管道的示例。请参阅《用户指南》。 连结多个特征提取方法 流水线：链接PCA和逻辑回归 混合类型的列转换器 使用Pipeline和GridSearchCV选择降维 具有异构数据源的列转换器 在回归模型中转换目标的效果 预处理 有关sklearn.preprocessing模块的示例。 使用FunctionTransformer选择列 使用KBinsDiscretizer离散化连续特征 演示KBinsDiscretizer的不同策略 特征缩放的重要性 地图数据正态分布 功能离散 比较不同缩放器对数据与异常值的影响 发布要点 这些示例说明了scikit-learn发行版的主要功能。 scikit-learn 0.22的发行要点 半监督分类 有关sklearn.semi_supervised模块的示例。 Iris数据集上标签传播与SVM的决策边界 标签传播学习复杂的结构 标签传播数字：演示性能 标签传播数字主动学习 支持向量机 有关sklearn.svm模块的示例。 非线性 SVM：最大余量分隔超平面 具有自定义内核的 在LinearSVC中绘制支持向量 SVM中断示例 SVM：加权样本 SVM：为不平衡的类分离超平面 SVM内核 SVM-Anova：具有单变量特征选择的 使用线性和非线性内核支持向量回归（SVR） SVM保证金示例 具有非线性内核（RBF）的一类 在虹膜数据集中绘制不同的SVM分类器 扩展SVC的正则化参数 RBF SVM参数 教程练习 教程练习。 数字分类练习 交叉验证数字数据集练习 SVM练习 糖尿病运动数据集交叉验证 文本文档工作 有关sklearn.feature_extraction.text模块的示例。 FeatureHasher和DictVectorizer比较 使用k-means聚类文本文档 使用稀疏特征对文本文档进行分类 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Biclustering/a_demo_of_the_spectral_co-clustering_algorithm.html":{"url":"Biclustering/a_demo_of_the_spectral_co-clustering_algorithm.html","title":"频谱共聚算法演示","keywords":"","body":"频谱共聚算法演示 翻译者：@N!no 校验者：待校验 这个例子演示了如何使用谱协聚类算法生成数据集并对其进行双聚类处理。 数据集是使用 make_biclusters 函数生成的，该函数创建一个小值矩阵，并将大值植入双聚类。然后将行和列打乱并传递给光谱协聚算法。通过重新排列变换后的矩阵可以使双聚类连续，这展示出该算法找到双聚类的准确性。 consensus score: 1.0 print(__doc__) # Author: Kemal Eren # License: BSD 3 clause import numpy as np from matplotlib import pyplot as plt from sklearn.datasets import make_biclusters from sklearn.cluster import SpectralCoclustering from sklearn.metrics import consensus_score data, rows, columns = make_biclusters( shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0) plt.matshow(data, cmap=plt.cm.Blues) plt.title(\"Original dataset\") # 打乱聚类的位置 rng = np.random.RandomState(0) row_idx = rng.permutation(data.shape[0]) col_idx = rng.permutation(data.shape[1]) data = data[row_idx][:, col_idx] plt.matshow(data, cmap=plt.cm.Blues) plt.title(\"Shuffled dataset\") model = SpectralCoclustering(n_clusters=5, random_state=0) model.fit(data) score = consensus_score(model.biclusters_, (rows[:, row_idx], columns[:, col_idx])) print(\"consensus score: {:.3f}\".format(score)) fit_data = data[np.argsort(model.row_labels_)] fit_data = fit_data[:, np.argsort(model.column_labels_)] plt.matshow(fit_data, cmap=plt.cm.Blues) plt.title(\"After biclustering; rearranged to show biclusters\") plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Biclustering/a_demo_of_the_spectral_clustering_algorithm.html":{"url":"Biclustering/a_demo_of_the_spectral_clustering_algorithm.html","title":"频谱双聚类算法演示","keywords":"","body":"频谱双聚类算法演示 翻译者：@N!no 校验者：待校验 这个例子演示了如何使用光谱聚类算法生成棋盘数据集并对其进行聚类处理。 数据是用make_checkerboard函数生成的，然后打乱顺序并传递给光谱双聚类算法。变换后的矩阵的行和列被重新排列，以显示该算法找到的双聚类。 行和列标签向量的外积表示棋盘结构。 consensus score: 1.0 print(__doc__) # Author: Kemal Eren # License: BSD 3 clause import numpy as np from matplotlib import pyplot as plt from sklearn.datasets import make_checkerboard from sklearn.cluster import SpectralBiclustering from sklearn.metrics import consensus_score n_clusters = (4, 3) data, rows, columns = make_checkerboard( shape=(300, 300), n_clusters=n_clusters, noise=10, shuffle=False, random_state=0) plt.matshow(data, cmap=plt.cm.Blues) plt.title(\"Original dataset\") # 打乱聚类顺序 rng = np.random.RandomState(0) row_idx = rng.permutation(data.shape[0]) col_idx = rng.permutation(data.shape[1]) data = data[row_idx][:, col_idx] plt.matshow(data, cmap=plt.cm.Blues) plt.title(\"Shuffled dataset\") model = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=0) model.fit(data) score = consensus_score(model.biclusters_, (rows[:, row_idx], columns[:, col_idx])) print(\"consensus score: {:.1f}\".format(score)) fit_data = data[np.argsort(model.row_labels_)] fit_data = fit_data[:, np.argsort(model.column_labels_)] plt.matshow(fit_data, cmap=plt.cm.Blues) plt.title(\"After biclustering; rearranged to show biclusters\") plt.matshow(np.outer(np.sort(model.row_labels_) + 1, np.sort(model.column_labels_) + 1), cmap=plt.cm.Blues) plt.title(\"Checkerboard structure of rearranged data\") plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Biclustering/biclustering_documents_with_the_spectral_co-clustering_algorithm.html":{"url":"Biclustering/biclustering_documents_with_the_spectral_co-clustering_algorithm.html","title":"使用频谱共聚算法对文档进行聚合","keywords":"","body":"使用频谱共聚算法对文档进行聚合 翻译者：@N!no 校验者：待校验 这个例子演示了20个新闻组数据集上的光谱协聚类算法。‘comp.os.ms-windows.misc’ 类别被排除在外，因为它包含许多只包含数据的帖子。 TF-IDF 矢量帖构成一个词频矩阵，然后使用 Dhillon 光谱协聚算法对其进行重组。由此产生的文档词双聚类表明在这些子集文档中被使用频率更高的子集词。 对于一些最好的双聚类来说，它最常见的文档类别和十个最重要的单词会被打印出来。最佳双类别由其归一化的切割决定。最好的单词是通过比较它们在两区内和两区外的总和来确定的。 为了进行比较，我们还使用 MiniBatchKMeans 对文档进行集群。从双聚类衍生出的文档聚类比使用 MiniBatchKMeans 得到的聚类具有更好的 V-measure。 Vectorizing... Coclustering... Done in 2.75s. V-measure: 0.4387 MiniBatchKMeans... Done in 5.69s. V-measure: 0.3344 Best biclusters: ---------------- bicluster 0 : 1829 documents, 2524 words categories : 22% comp.sys.ibm.pc.hardware, 19% comp.sys.mac.hardware, 18% comp.graphics words : card, pc, ram, drive, bus, mac, motherboard, port, windows, floppy bicluster 1 : 2391 documents, 3275 words categories : 18% rec.motorcycles, 17% rec.autos, 15% sci.electronics words : bike, engine, car, dod, bmw, honda, oil, motorcycle, behanna, ysu bicluster 2 : 1887 documents, 4232 words categories : 23% talk.politics.guns, 19% talk.politics.misc, 13% sci.med words : gun, guns, firearms, geb, drugs, banks, dyer, amendment, clinton, cdt bicluster 3 : 1146 documents, 3263 words categories : 29% talk.politics.mideast, 26% soc.religion.christian, 25% alt.atheism words : god, jesus, christians, atheists, kent, sin, morality, belief, resurrection, marriage bicluster 4 : 1732 documents, 3967 words categories : 26% sci.crypt, 23% sci.space, 17% sci.med words : clipper, encryption, key, escrow, nsa, crypto, keys, intercon, secure, wiretap from collections import defaultdict import operator from time import time import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.cluster import MiniBatchKMeans from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.cluster import v_measure_score print(__doc__) def number_normalizer(tokens): \"\"\" 将所有数字标记映射到占位符。 对于许多应用程序来说，以数字开头的令牌并没有直接的用处，但是这样的令牌存在的事实可能是相关的。通过应用这种降维形式，一些方法可能会表现得更好。 \"\"\" return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens) class NumberNormalizingVectorizer(TfidfVectorizer): def build_tokenizer(self): tokenize = super().build_tokenizer() return lambda doc: list(number_normalizer(tokenize(doc))) # 不包含 'comp.os.ms-windows.misc' 类别 categories = ['alt.atheism', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] newsgroups = fetch_20newsgroups(categories=categories) y_true = newsgroups.target vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=5) cocluster = SpectralCoclustering(n_clusters=len(categories), svd_method='arpack', random_state=0) kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000, random_state=0) print(\"Vectorizing...\") X = vectorizer.fit_transform(newsgroups.data) print(\"Coclustering...\") start_time = time() cocluster.fit(X) y_cocluster = cocluster.row_labels_ print(\"Done in {:.2f}s. V-measure: {:.4f}\".format( time() - start_time, v_measure_score(y_cocluster, y_true))) print(\"MiniBatchKMeans...\") start_time = time() y_kmeans = kmeans.fit_predict(X) print(\"Done in {:.2f}s. V-measure: {:.4f}\".format( time() - start_time, v_measure_score(y_kmeans, y_true))) feature_names = vectorizer.get_feature_names() document_names = list(newsgroups.target_names[i] for i in newsgroups.target) def bicluster_ncut(i): rows, cols = cocluster.get_indices(i) if not (np.any(rows) and np.any(cols)): import sys return sys.float_info.max row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0] col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0] # 注意：接下来的操作等同于 X[rows[:, np.newaxis], cols].sum() # 但是会针对于 scipy = 2.7 中类似于 Counter.most_common 。 \"\"\" return sorted(d.items(), key=operator.itemgetter(1), reverse=True) bicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names))) best_idx = np.argsort(bicluster_ncuts)[:5] print() print(\"Best biclusters:\") print(\"----------------\") for idx, cluster in enumerate(best_idx): n_rows, n_cols = cocluster.get_shape(cluster) cluster_docs, cluster_words = cocluster.get_indices(cluster) if not len(cluster_docs) or not len(cluster_words): continue # 种类 counter = defaultdict(int) for i in cluster_docs: counter[document_names[i]] += 1 cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name) for name, c in most_common(counter)[:3]) # 单词 out_of_cluster_docs = cocluster.row_labels_ != cluster out_of_cluster_docs = np.where(out_of_cluster_docs)[0] word_col = X[:, cluster_words] word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) - word_col[out_of_cluster_docs, :].sum(axis=0)) word_scores = word_scores.ravel() important_words = list(feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]) print(\"bicluster {} : {} documents, {} words\".format( idx, n_rows, n_cols)) print(\"categories : {}\".format(cat_string)) print(\"words : {}\\n\".format(', '.join(important_words))) 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_lasso_and_elasticnet.html":{"url":"Generalized_Linear_Models/plot_lasso_and_elasticnet.html","title":"Lasso 和弹性网络在稀疏信号上的表现","keywords":"","body":"Lasso和Elastic Net(弹性网络)在稀疏信号上的表现 翻译者:@Loopy校验者:@barrycg 评估了Lasso回归模型和弹性网络回归模型在手动生成的，并附加噪声的稀疏信号上的表现，并将回归系数与真实值进行了比较。 import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import r2_score # 产生一些稀疏值 np.random.seed(42) n_samples, n_features = 50, 100 X = np.random.randn(n_samples, n_features) # 减少交替出现的符号以使其便于可视化 idx = np.arange(n_features) coef = (-1) ** idx * np.exp(-idx / 10) coef[10:] = 0 # sparsify coef y = np.dot(X, coef) # 添加噪音 y += 0.01 * np.random.normal(size=n_samples) # 划分测试,训练集 n_samples = X.shape[0] X_train, y_train = X[:n_samples // 2], y[:n_samples // 2] X_test, y_test = X[n_samples // 2:], y[n_samples // 2:] # Lasso from sklearn.linear_model import Lasso alpha = 0.1 lasso = Lasso(alpha=alpha) y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test) r2_score_lasso = r2_score(y_test, y_pred_lasso) print(lasso) print(\"r^2 on test data : %f\" % r2_score_lasso) Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.658064 # 弹性网络(ElasticNet) from sklearn.linear_model import ElasticNet enet = ElasticNet(alpha=alpha, l1_ratio=0.7) y_pred_enet = enet.fit(X_train, y_train).predict(X_test) r2_score_enet = r2_score(y_test, y_pred_enet) print(enet) print(\"r^2 on test data : %f\" % r2_score_enet) ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.642515 m, s, _ = plt.stem(np.where(enet.coef_)[0], enet.coef_[enet.coef_ != 0], markerfmt='x', label='Elastic net系数') plt.setp([m, s], color=\"#2ca02c\") m, s, _ = plt.stem(np.where(lasso.coef_)[0], lasso.coef_[lasso.coef_ != 0], markerfmt='x', label='Lasso系数') plt.setp([m, s], color='#ff7f0e') plt.stem(np.where(coef)[0], coef[coef != 0], label='真实系数', markerfmt='bx') plt.legend(loc='best') plt.title(\"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" % (r2_score_lasso, r2_score_enet)) plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_lasso_coordinate_descent_path.html":{"url":"Generalized_Linear_Models/plot_lasso_coordinate_descent_path.html","title":"Lasso 和弹性网络","keywords":"","body":"Lasso和Elastic Net(弹性网络) 翻译者:@Loopy校验者:@barrycg 使用坐标下降实现Lasso和Elastic Net(弹性网络)(L1和L2罚项)。 回归系数可以被强制设定为正。 from itertools import cycle import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import lasso_path, enet_path from sklearn import datasets diabetes = datasets.load_diabetes() X = diabetes.data y = diabetes.target X /= X.std(axis=0) # 标准化数据(使得l1_ratio参数更容易被设置) # 计算路径 eps = 5e-3 # 它越小，路径就越长 print(\"使用lasso计算正则化路径...\") alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False) print(\"使用系数强制为正的lasso计算正则化路径...\") alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path( X, y, eps, positive=True, fit_intercept=False) print(\"使用Elastic Net计算正则化路径...\") alphas_enet, coefs_enet, _ = enet_path( X, y, eps=eps, l1_ratio=0.8, fit_intercept=False) print(\"使用系数强制为正的Elastic Net计算正则化路径...\") alphas_positive_enet, coefs_positive_enet, _ = enet_path( X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False) 使用lasso计算正则化路径... 使用系数强制为正的lasso计算正则化路径... 使用Elastic Net计算正则化路径... 使用系数强制为正的Elastic Net计算正则化路径... # 输出结果 plt.figure(1) colors = cycle(['b', 'r', 'g', 'c', 'k']) neg_log_alphas_lasso = -np.log10(alphas_lasso) neg_log_alphas_enet = -np.log10(alphas_enet) for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors): l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c) l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c) plt.xlabel('-Log(alpha)') plt.ylabel('系数') plt.title('Lasso和Elastic-Net的路径') plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left') plt.axis('tight') (-1.8715612353951188, 0.7661727741441225, -14.885251328819376, 28.52127146246762) plt.figure(2) neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso) for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors): l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c) l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c) plt.xlabel('-Log(alpha)') plt.ylabel('系数') plt.title('Lasso和系数强制为正的lasso的路径') plt.legend((l1[-1], l2[-1]), ('Lasso', '系数强制为正的Lasso'), loc='lower left') plt.axis('tight') (-1.7698057217366594, 0.7613272734937198, -14.945497919817987, 29.786449873438457) plt.figure(3) neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet) for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors): l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c) l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c) plt.xlabel('-Log(alpha)') plt.ylabel('系数') plt.title('Elastic-Net和系数强制为正的Elastic-Net的路径') plt.legend((l1[-1], l2[-1]), ('Elastic-Net', '系数强制为正的Elastic-Net'), loc='lower left') plt.axis('tight') plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_lasso_model_selection.html":{"url":"Generalized_Linear_Models/plot_lasso_model_selection.html","title":"Lasso 模型选择：交叉验证 / AIC / BIC","keywords":"","body":"Lasso模型选择:交叉验证 / AIC / BIC 翻译者:@Loopy校验者:@barrycg 本示例利用Akaike信息判据(AIC)、Bayes信息判据(BIC)和交叉验证，来筛选Lasso回归的正则化项参数alpha的最优值。 通过LassoLarsIC得到的结果，是基于AIC/BIC判据的。 这种基于信息判据(AIC/BIC)的模型选择非常快，但它依赖于对自由度的正确估计。该方式的假设模型必需是正确, 而且是对大样本(渐近结果)进行推导，即，数据实际上是由该模型生成的。当问题的背景条件很差时(特征数大于样本数)，该模型选择方式会崩溃。 对于交叉验证，我们使用20-fold的2种算法来计算Lasso路径:LassoCV类实现的坐标下降和LassoLarsCV类实现的最小角度回归(Lars)。这两种算法给出的结果大致相同,但它们在执行速度和数值误差来源方面有所不同。 Lars仅为路径中的每个拐点计算路径解决方案。因此，当只有很少的弯折时，也就是很少的特征或样本时，它是非常有效的。此外，它能够计算完整的路径，而不需要设置任何元参数。与之相反，坐标下降算法计算预先指定的网格上的路径点(本示例中我们使用缺省值)。因此，如果网格点的数量小于路径中的拐点的数量，则效率更高。如果特征数量非常大，并且有足够的样本来选择大量特征，那么这种策略就非常有趣。在数值误差方面，Lars会因变量间的高相关度而积累更多的误差，而坐标下降算法只会采样网格上路径。 注意观察alpha的最优值是如何随着每个fold而变化。这是为什么当估交叉验证选择参数的方法的性能时，需要使用嵌套交叉验证的原因:这种参数的选择对于不可见数据可能不是最优的。 import time import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC from sklearn import datasets # 这样做是为了避免在np.log10时除零 EPSILON = 1e-4 diabetes = datasets.load_diabetes() X = diabetes.data y = diabetes.target rng = np.random.RandomState(42) X = np.c_[X, rng.randn(X.shape[0], 14)] # 添加一些不好的特征 # 将最小角度回归得到的数据标准化，以便进行比较 X /= np.sqrt(np.sum(X ** 2, axis=0)) # LassoLarsIC: 用BIC/AIC判据进行最小角度回归 model_bic = LassoLarsIC(criterion='bic') t1 = time.time() model_bic.fit(X, y) t_bic = time.time() - t1 alpha_bic_ = model_bic.alpha_ model_aic = LassoLarsIC(criterion='aic') model_aic.fit(X, y) alpha_aic_ = model_aic.alpha_ def plot_ic_criterion(model, name, color): alpha_ = model.alpha_ + EPSILON alphas_ = model.alphas_ + EPSILON criterion_ = model.criterion_ plt.plot(-np.log10(alphas_), criterion_, '--', color=color, linewidth=3, label='%s 判据' % name) plt.axvline(-np.log10(alpha_), color=color, linewidth=3, label='alpha: %s 估计' % name) plt.xlabel('-log(alpha)') plt.ylabel('判据') plt.figure() plot_ic_criterion(model_aic, 'AIC', 'b') plot_ic_criterion(model_bic, 'BIC', 'r') plt.legend() plt.title('模型选择的信息判据 (训练时间:%.3fs)' % t_bic) Text(0.5, 1.0, '模型选择的信息判据 (训练时间:0.024s)') # LassoCV: 坐标下降 # 计算路径 t1 = time.time() model = LassoCV(cv=20).fit(X, y) t_lasso_cv = time.time() - t1 # 显示结果 m_log_alphas = -np.log10(model.alphas_ + EPSILON) plt.figure() ymin, ymax = 2300, 3800 plt.plot(m_log_alphas, model.mse_path_, ':') plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k', label='平均', linewidth=2) plt.axvline(-np.log10(model.alpha_ + EPSILON), linestyle='--', color='k', label='alpha:CV估计') plt.legend() plt.xlabel('-log(alpha)') plt.ylabel('均方惨差') plt.title('每折上的均方残差: 坐标下降法' '(训练时间: %.2fs)' % t_lasso_cv) plt.axis('tight') plt.ylim(ymin, ymax) (2300, 3800) # LassoLarsCV: 最小角度回归法 # Compute paths print(\"Computing regularization path using the Lars lasso...\") t1 = time.time() model = LassoLarsCV(cv=20).fit(X, y) t_lasso_lars_cv = time.time() - t1 # Display results m_log_alphas = -np.log10(model.cv_alphas_ + EPSILON) plt.figure() plt.plot(m_log_alphas, model.mse_path_, ':') plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k', label='平均', linewidth=2) plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k', label='alpha CV') plt.legend() plt.xlabel('-log(alpha)') plt.ylabel('均方惨差') plt.title('每折上的均方残差: 最小角度回归法' '(训练时间: %.2fs)' % t_lasso_lars_cv) plt.axis('tight') plt.ylim(ymin, ymax) plt.show() Computing regularization path using the Lars lasso... 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_multi_task_lasso_support.html":{"url":"Generalized_Linear_Models/plot_multi_task_lasso_support.html","title":"多任务 Lasso 实现联合特征选择","keywords":"","body":"多任务Lasso实现联合特征选择 翻译者:@Loopy校验者:@barrycg 多任务lasso允许多元回归问题上进行合并训练，并在多个任务间强制选择相同的特征。这个示例模拟了部分序列测量，每个任务都是即时的，并且相关的特征幅值趋向相同时,又会随时间变化而震动。多任务lasso强制要求在一个时间点选择的特征必需适用于所有时间点。这使得多任务LASSO的特征选择更加稳定。 import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import MultiTaskLasso, Lasso rng = np.random.RandomState(42) # 使用具有随机频率和相位的正弦波生成二维系数 n_samples, n_features, n_tasks = 100, 30, 40 n_relevant_features = 5 coef = np.zeros((n_tasks, n_features)) times = np.linspace(0, 2 * np.pi, n_tasks) for k in range(n_relevant_features): coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1)) X = rng.randn(n_samples, n_features) Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks) coef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T]) coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_ fig = plt.figure(figsize=(8, 5)) plt.subplot(1, 2, 1) plt.spy(coef_lasso_) plt.xlabel('特征') plt.ylabel('时间 (或者叫 任务)') plt.text(10, 5, 'Lasso') plt.subplot(1, 2, 2) plt.spy(coef_multi_task_lasso_) plt.xlabel('特征') plt.ylabel('时间 (或者叫 任务)') plt.text(10, 5, '多任务Lasso') fig.suptitle('系数非零的位置') Text(0.5, 0.98, '系数非零的位置') feature_to_plot = 0 plt.figure() lw = 2 plt.plot(coef[:, feature_to_plot], color='seagreen', linewidth=lw, label='真实值') plt.plot(coef_lasso_[:, feature_to_plot], color='cornflowerblue', linewidth=lw, label='Lasso') plt.plot(coef_multi_task_lasso_[:, feature_to_plot], color='gold', linewidth=lw, label='多任务Lasso') plt.legend(loc='upper center') plt.axis('tight') plt.ylim([-1.1, 1.1]) plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_ols.html":{"url":"Generalized_Linear_Models/plot_ols.html","title":"线性回归示例","keywords":"","body":"线性回归 翻译者:@Loopy校验者:@barrycg 本例仅使用糖尿病数据集的第一个特征，来展示线性回归在二维空间上的表现。下图中的直线, 即是线性回归所确定的一个界限，其目标是使得数据集中的实际值与线性回归所得的预测值之间的残差平方和最小。 同时也计算了回归系数、残差平方和以及解释方差得分，来判断该线性回归模型的质量。 原文解释和代码不符合: 实际上计算了回归系数, 均方误差（MSE）,判定系数(r2_score) 判定系数和解释方差得分并不绝对相等，当实际值和预测值之间的误差均值为0时，两者相同，否则数值上有着略微误差。均方误差和残差平方和虽然都是定义预测值和实际值的偏离关系，但是数值大小的差距较大。 解释方差得分函数: explained_variance_score()。 import matplotlib.pyplot as plt import numpy as np from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score # 加载糖尿病数据集 diabetes = datasets.load_diabetes() # 只使用1个特征 diabetes_X = diabetes.data[:, np.newaxis, 2] # 划分训练,测试集 diabetes_X_train = diabetes_X[:-20] diabetes_X_test = diabetes_X[-20:] diabetes_y_train = diabetes.target[:-20] diabetes_y_test = diabetes.target[-20:] # 创建线性回归模型 regr = linear_model.LinearRegression() # 训练这个模型 regr.fit(diabetes_X_train, diabetes_y_train) # 使用模型做预测 diabetes_y_pred = regr.predict(diabetes_X_test) # 评价预测结果 print('回归系数(coef_):', regr.coef_) print(\"均方误差(MSE): %.2f\"% mean_squared_error(diabetes_y_test, diabetes_y_pred)) print('判断系数(r2_score): %.2f' % r2_score(diabetes_y_test, diabetes_y_pred)) # 额外增加 print('解释方差得分(explained_variance_score): %.2f' % explained_variance_score(diabetes_y_test, diabetes_y_pred)) 回归系数(coef_): [938.23786125] 均方误差(MSE): 2548.07 判断系数(r2_score): 0.47 解释方差得分(explained_variance_score): 0.53 # 绘制输出 plt.scatter(diabetes_X_test, diabetes_y_test, color='black') plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3) plt.xticks(()) plt.yticks(()) plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_ridge_path.html":{"url":"Generalized_Linear_Models/plot_ridge_path.html","title":"岭系数对回归系数的影响","keywords":"","body":"岭系数对回归系数的影响 翻译者:@Loopy校验者:@barrycg 本实例展示了回归模型系数间的共线性。 岭回归 是本例中使用的预测模型。每种颜色都分别表示不同特征下的回归系数向量，岭系数alpha是正则化项的参数。下图表示了岭系数作为变量参数,回归系数在岭回归模型中的变化。 这个示例还显示了将岭回归应用于高病态矩阵的有效性。在高病态矩阵中，一些变量的微小变化会导致计算权重(回归系数)的巨大差异。在这种情况下，设置一个确定的正则化项(alpha)可以减少这种不良差异(噪声)。 当alpha很大时，正则化项是平方损失函数的主要影响因子，回归系数的影响趋于零。在路径的末端，当alpha趋近零时,损失函数会慢慢变成普通最小二乘，回归系数表现出较大的波动。在实践中，有必要对alpha进行调优，以便在两者之间保持平衡。 import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # X 是10x10的希尔伯特矩阵 X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis]) y = np.ones(10) # 计算不同岭系数时的回归系数 n_alphas = 200 alphas = np.logspace(-10, -2, n_alphas) coefs = [] for a in alphas: ridge = linear_model.Ridge(alpha=a, fit_intercept=False) ridge.fit(X, y) coefs.append(ridge.coef_) #绘图 ax = plt.gca() ax.plot(alphas, coefs) ax.set_xscale('log') ax.set_xlim(ax.get_xlim()[::-1]) plt.xlabel('岭系数alpha') plt.ylabel('回归系数coef_') plt.title('岭系数对回归系数的影响') plt.axis('tight') plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_tomography_l1_reconstruction.html":{"url":"Generalized_Linear_Models/plot_tomography_l1_reconstruction.html","title":"压缩感知：用L1先验概率进行断层重建","keywords":"","body":"压缩感知_断层重建 翻译者:@Loopy校验者:@barrycg 这个示例展示了从一组沿不同角度获得的平行投影来重建图像的过程。这样的数据集是在CT(计算机断层扫描)中获得的。 在没有任何样本得先验信息的情况下，重建图像所需的投影数与图像的线性大小l(以像素为单位)相同。为了简单起见，我们在这里考虑稀疏图像，其中只有对象边界上的像素具有非零值(例如:这些数据可以对应于细胞材料)。但是请注意，大多数图像在不同的基(basis)上是稀疏的，比如Haar小波 。只获得了l/7的投影，因此有必要利用关于样品的现有信息(稀疏性):这是压缩感知的一个示例。 层析投影操作是一种线性变换。除了线性回归对应的数据保真项外，我们还对图像的L1范数进行了惩罚，以解释其稀疏性。由此产生的优化问题称为Lasso。我们使用类sklearn.linear_model.Lasso,它是使用坐标下降算法实现的。重要的是，这种在稀疏阵上算法的计算效率比这里投影算子更高。 即使在投影中添加了噪声,L1罚项重建得到的结果也会是零误差(所有像素都被成功地标记为0或1)。相比之下，L2罚项(sklearn.linear_model.Ridge)会产生大量标记错误,也就是在重构图像上会观察到伪影，这与L1罚项相反。特别要注意的是，角落里分隔像素的圆形伪影所形成的投影比中央部分少。 import numpy as np from scipy import sparse from scipy import ndimage from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge import matplotlib.pyplot as plt def _weights(x, dx=1, orig=0): x = np.ravel(x) floor_x = np.floor((x - orig) / dx).astype(np.int64) alpha = (x - orig - floor_x * dx) / dx return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha)) def _generate_center_coordinates(l_x): X, Y = np.mgrid[:l_x, :l_x].astype(np.float64) center = l_x / 2. X += 0.5 - center Y += 0.5 - center return X, Y def build_projection_operator(l_x, n_dir): \"\"\" 计算层析矩阵 参数 ---------- l_x : int 图像阵列的线性大小 n_dir : int 投影的角度数 Returns ------- p : shape为(n_dir l_x, l_x**2)的稀疏矩阵 \"\"\" X, Y = _generate_center_coordinates(l_x) angles = np.linspace(0, np.pi, n_dir, endpoint=False) data_inds, weights, camera_inds = [], [], [] data_unravel_indices = np.arange(l_x ** 2) data_unravel_indices = np.hstack((data_unravel_indices, data_unravel_indices)) for i, angle in enumerate(angles): Xrot = np.cos(angle) * X - np.sin(angle) * Y inds, w = _weights(Xrot, dx=1, orig=X.min()) mask = np.logical_and(inds >= 0, inds def generate_synthetic_data(): \"\"\" 合成二进制数据 \"\"\" rs = np.random.RandomState(0) n_pts = 36 x, y = np.ogrid[0:l, 0:l] mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 mask.mean(), mask_outer) return np.logical_xor(res, ndimage.binary_erosion(res)) # 生成合成图像和投影 l = 128 proj_operator = build_projection_operator(l, l // 7) data = generate_synthetic_data() proj = proj_operator * data.ravel()[:, np.newaxis] proj += 0.15 * np.random.randn(*proj.shape) # 用L2(岭)罚项重建 rgr_ridge = Ridge(alpha=0.2) rgr_ridge.fit(proj_operator, proj.ravel()) rec_l2 = rgr_ridge.coef_.reshape(l, l) # 用L1(Lasso)罚项重建 # 采用交叉验证法确定最佳值 rgr_lasso = Lasso(alpha=0.001) rgr_lasso.fit(proj_operator, proj.ravel()) rec_l1 = rgr_lasso.coef_.reshape(l, l) #画图 plt.figure(figsize=(8, 3.3)) plt.subplot(131) plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest') plt.axis('off') plt.title('原始图像') plt.subplot(132) plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest') plt.title('L2罚项') plt.axis('off') plt.subplot(133) plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest') plt.title('L1罚项') plt.axis('off') plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0, right=1) plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"Generalized_Linear_Models/plot_document_classification_20newsgroups.html":{"url":"Generalized_Linear_Models/plot_document_classification_20newsgroups.html","title":"分类特征稀疏的文本","keywords":"","body":"分类特征稀疏的文本 翻译者:@Loopy校验者:@barrycg 这个示例展示了如何使用scikit-learn中的单词包方法，根据主题对文档进行分类。本例使用scipy.sparse中的矩阵来存储特征，并演示各种能够有效处理稀疏矩阵的分类器。 本例中使用的数据集是20条新闻组数据集。通过scikit-learn可以自动下载该数据集，并进行缓存。 下述条形图展示了各个不同分类器，其信息包括精度、训练时间(已归一化)和测试时间(已归一化)。 import logging import numpy as np from optparse import OptionParser import sys from time import time import matplotlib.pyplot as plt from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction.text import HashingVectorizer from sklearn.feature_selection import SelectFromModel from sklearn.feature_selection import SelectKBest, chi2 from sklearn.linear_model import RidgeClassifier from sklearn.pipeline import Pipeline from sklearn.svm import LinearSVC from sklearn.linear_model import SGDClassifier from sklearn.linear_model import Perceptron from sklearn.linear_model import PassiveAggressiveClassifier from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import NearestCentroid from sklearn.ensemble import RandomForestClassifier from sklearn.utils.extmath import density from sklearn import metrics # 在stdout上显示进度日志 logging.basicConfig(level=logging.INFO,format='%(asctime)s %(levelname)s %(message)s') # 解析命令行参数 op = OptionParser() op.add_option(\"--report\", action=\"store_true\", dest=\"print_report\", help=\"Print a detailed classification report.\") op.add_option(\"--chi2_select\", action=\"store\", type=\"int\", dest=\"select_chi2\", help=\"Select some number of features using a chi-squared test\") op.add_option(\"--confusion_matrix\", action=\"store_true\", dest=\"print_cm\", help=\"Print the confusion matrix.\") op.add_option(\"--top10\", action=\"store_true\", dest=\"print_top10\", help=\"Print ten most discriminative terms per class\" \" for every classifier.\") op.add_option(\"--all_categories\", action=\"store_true\", dest=\"all_categories\", help=\"Whether to use all categories or not.\") op.add_option(\"--use_hashing\", action=\"store_true\", help=\"Use a hashing vectorizer.\") op.add_option(\"--n_features\", action=\"store\", type=int, default=2 ** 16, help=\"n_features when using the hashing vectorizer.\") op.add_option(\"--filtered\", action=\"store_true\", help=\"Remove newsgroup information that is easily overfit: \" \"headers, signatures, and quoting.\") def is_interactive(): return not hasattr(sys.modules['__main__'], '__file__') # Jupyter notebook上的运行方法 argv = [] if is_interactive() else sys.argv[1:] (opts, args) = op.parse_args(argv) if len(args) > 0: op.error(\"this script takes no arguments.\") sys.exit(1) print(__doc__) op.print_help() print() Automatically created module for IPython interactive environment Usage: ipykernel_launcher.py [options] Options: -h, --help show this help message and exit --report Print a detailed classification report. --chi2_select=SELECT_CHI2 Select some number of features using a chi-squared test --confusion_matrix Print the confusion matrix. --top10 Print ten most discriminative terms per class for every classifier. --all_categories Whether to use all categories or not. --use_hashing Use a hashing vectorizer. --n_features=N_FEATURES n_features when using the hashing vectorizer. --filtered Remove newsgroup information that is easily overfit: headers, signatures, and quoting. # 从训练集中加载一些类别 if opts.all_categories: categories = None else: categories = [ 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', ] if opts.filtered: remove = ('headers', 'footers', 'quotes') else: remove = () print(\"Loading 20 newsgroups dataset for categories:\") print(categories if categories else \"all\") Loading 20 newsgroups dataset for categories: ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'] # 下载数据集 data_train = fetch_20newsgroups(subset='train', categories=categories,shuffle=True, random_state=42,remove=remove) data_test = fetch_20newsgroups(subset='test', categories=categories,shuffle=True, random_state=42,remove=remove) # target_names中的标签顺序可以与categories中的不同 target_names = data_train.target_names def size_mb(docs): return sum(len(s.encode('utf-8')) for s in docs) / 1e6 data_train_size_mb = size_mb(data_train.data) data_test_size_mb = size_mb(data_test.data) print(\"%d documents - %0.3fMB (training set)\" % ( len(data_train.data), data_train_size_mb)) print(\"%d documents - %0.3fMB (test set)\" % ( len(data_test.data), data_test_size_mb)) print(\"%d categories\" % len(target_names)) 2034 documents - 3.980MB (training set) 1353 documents - 2.867MB (test set) 4 categories # 划分测试,训练集 y_train, y_test = data_train.target, data_test.target print(\"使用稀疏向量机从训练数据中提取特征\") t0 = time() if opts.use_hashing: vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False, n_features=opts.n_features) X_train = vectorizer.transform(data_train.data) else: vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english') X_train = vectorizer.fit_transform(data_train.data) duration = time() - t0 print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration)) print(\"n_samples: %d, n_features: %d\" % X_train.shape) 使用稀疏向量机从训练数据中提取特征 done in 0.476004s at 8.360MB/s n_samples: 2034, n_features: 33809 print(\"使用相同的矢量化器从测试数据中提取特征\") t0 = time() X_test = vectorizer.transform(data_test.data) duration = time() - t0 print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration)) print(\"n_samples: %d, n_features: %d\" % X_test.shape) 使用相同的矢量化器从测试数据中提取特征 done in 0.311447s at 9.207MB/s n_samples: 1353, n_features: 33809 # 从整数的特征名称映射到原始的token字符串 if opts.use_hashing: feature_names = None else: feature_names = vectorizer.get_feature_names() if opts.select_chi2: print(\"使用卡方检验提取 %d 个特征\" % opts.select_chi2) t0 = time() ch2 = SelectKBest(chi2, k=opts.select_chi2) X_train = ch2.fit_transform(X_train, y_train) X_test = ch2.transform(X_test) if feature_names: # keep selected feature names feature_names = [feature_names[i] for i in ch2.get_support(indices=True)] print(\"done in %fs\" % (time() - t0)) if feature_names: feature_names = np.asarray(feature_names) # 修剪字符串以适应终端(假设显示80列) def trim(s): return s if len(s) 基准分类器 def benchmark(clf): print('_' * 80) print(\"训练: \") print(clf) t0 = time() clf.fit(X_train, y_train) train_time = time() - t0 print(\"训练时间: %0.3fs\" % train_time) t0 = time() pred = clf.predict(X_test) test_time = time() - t0 print(\"最佳时间: %0.3fs\" % test_time) score = metrics.accuracy_score(y_test, pred) print(\"准确率: %0.3f\" % score) if hasattr(clf, 'coef_'): print(\"维数: %d\" % clf.coef_.shape[1]) print(\"密度: %f\" % density(clf.coef_)) if opts.print_top10 and feature_names is not None: print(\"每个类的前十个词:\") for i, label in enumerate(target_names): top10 = np.argsort(clf.coef_[i])[-10:] print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10])))) print() if opts.print_report: print(\"分类报告:\") print(metrics.classification_report(y_test, pred, target_names=target_names)) if opts.print_cm: print(\"混淆矩阵:\") print(metrics.confusion_matrix(y_test, pred)) clf_descr = str(clf).split('(')[0] return clf_descr, score, train_time, test_time results = [] for clf, name in ( (RidgeClassifier(tol=1e-2, solver=\"sag\"), \"岭分类器\"), (Perceptron(max_iter=50, tol=1e-3), \"感知器\"), (PassiveAggressiveClassifier(max_iter=50, tol=1e-3), \"PAC分类器\"), (KNeighborsClassifier(n_neighbors=10), \"K近邻\"), (RandomForestClassifier(n_estimators=100), \"随机森林\")): print('=' * 80) print(name) results.append(benchmark(clf)) ================================================================================ 岭分类器 ________________________________________________________________________________ 训练: RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='sag', tol=0.01) 训练时间: 0.202s 最佳时间: 0.002s 准确率: 0.897 维数: 33809 密度: 1.000000 ================================================================================ 感知器 ________________________________________________________________________________ 训练: Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0, fit_intercept=True, max_iter=50, n_iter_no_change=5, n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 训练时间: 0.030s 最佳时间: 0.003s 准确率: 0.888 维数: 33809 密度: 0.255302 ================================================================================ PAC分类器 ________________________________________________________________________________ 训练: PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None, early_stopping=False, fit_intercept=True, loss='hinge', max_iter=50, n_iter_no_change=5, n_jobs=None, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 训练时间: 0.063s 最佳时间: 0.003s 准确率: 0.902 维数: 33809 密度: 0.700487 ================================================================================ K近邻 ________________________________________________________________________________ 训练: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=10, p=2, weights='uniform') 训练时间: 0.002s 最佳时间: 0.235s 准确率: 0.858 ================================================================================ 随机森林 ________________________________________________________________________________ 训练: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) 训练时间: 1.752s 最佳时间: 0.084s 准确率: 0.822 for penalty in [\"l2\", \"l1\"]: print('=' * 80) print(\"%s 罚项\" % penalty.upper()) # 训练Liblinear模型 results.append(benchmark(LinearSVC(penalty=penalty, dual=False, tol=1e-3))) # 训练SGD model模型 results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty))) ================================================================================ L2 罚项 ________________________________________________________________________________ 训练: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.001, verbose=0) 训练时间: 0.274s 最佳时间: 0.003s 准确率: 0.900 维数: 33809 密度: 1.000000 ________________________________________________________________________________ 训练: SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50, n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 训练时间: 0.050s 最佳时间: 0.003s 准确率: 0.899 维数: 33809 密度: 0.573353 ================================================================================ L1 罚项 ________________________________________________________________________________ 训练: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l1', random_state=None, tol=0.001, verbose=0) 训练时间: 0.257s 最佳时间: 0.002s 准确率: 0.873 维数: 33809 密度: 0.005568 ________________________________________________________________________________ 训练: SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50, n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 训练时间: 0.187s 最佳时间: 0.003s 准确率: 0.882 维数: 33809 密度: 0.023049 # 训练带弹性网络(Elastic Net)罚项的SGD模型 print('=' * 80) print(\"弹性网络(Elastic Net)罚项\") results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty=\"elasticnet\"))) ================================================================================ 弹性网络(Elastic Net)罚项 ________________________________________________________________________________ 训练: SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50, n_iter_no_change=5, n_jobs=None, penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 训练时间: 0.295s 最佳时间: 0.003s 准确率: 0.897 维数: 33809 密度: 0.185956 # 训练不带阈值的Rocchio分类器 print('=' * 80) print(\"不带阈值的Rocchio分类器\") results.append(benchmark(NearestCentroid())) ================================================================================ 不带阈值的Rocchio分类器 ________________________________________________________________________________ 训练: NearestCentroid(metric='euclidean', shrink_threshold=None) 训练时间: 0.007s 最佳时间: 0.002s 准确率: 0.855 # 训练稀疏朴素贝叶斯分类器 print('=' * 80) print(\"稀疏朴素贝叶斯分类器\") results.append(benchmark(MultinomialNB(alpha=.01))) results.append(benchmark(BernoulliNB(alpha=.01))) results.append(benchmark(ComplementNB(alpha=.1))) ================================================================================ 稀疏朴素贝叶斯分类器 ________________________________________________________________________________ 训练: MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True) 训练时间: 0.007s 最佳时间: 0.003s 准确率: 0.899 维数: 33809 密度: 1.000000 ________________________________________________________________________________ 训练: BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True) 训练时间: 0.010s 最佳时间: 0.008s 准确率: 0.884 维数: 33809 密度: 1.000000 ________________________________________________________________________________ 训练: ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False) 训练时间: 0.007s 最佳时间: 0.002s 准确率: 0.911 维数: 33809 密度: 1.000000 print('=' * 80) print(\"基于l1的特征选择的LinearSVC\") # The smaller C, the stronger the regularization. # The more regularization, the more sparsity. results.append(benchmark(Pipeline([ ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, tol=1e-3))), ('classification', LinearSVC(penalty=\"l2\"))]))) ================================================================================ 基于l1的特征选择的LinearSVC ________________________________________________________________________________ 训练: Pipeline(memory=None, steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l1', random_state=None, tol=0.001, verbose=0), max_features=None, norm_order=1, prefit=False, threshold=None)), ('classification', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0))], verbose=False) 训练时间: 0.277s 最佳时间: 0.002s 准确率: 0.880 # 参考翻译 classifier_dic={ 'RidgeClassifier':'岭分类器(Ridge)', 'Perceptron':'感知器(Perceptron)', 'PassiveAggressiveClassifier':'PAC分类器', 'KNeighborsClassifier':'K近邻(KNN)', 'RandomForestClassifier':'随机森林', 'LinearSVC':'线性SVC', 'SGDClassifier':'SGD分类器', 'NearestCentroid':'线性SVC', 'MultinomialNB':'(多项式)稀疏朴素贝叶斯分类器', 'BernoulliNB':'(伯努利)稀疏朴素贝叶斯分类器', 'ComplementNB':'(补偿)稀疏朴素贝叶斯分类器', 'Pipeline':'基于l1的特征选择的LinearSVC', } # 绘图 indices = np.arange(len(results)) results = [[x[i] for x in results] for i in range(4)] clf_names, score, training_time, test_time = results training_time = np.array(training_time) / np.max(training_time) test_time = np.array(test_time) / np.max(test_time) plt.figure(figsize=(12, 8)) plt.title(\"模型对比\") plt.barh(indices, score, .2, label=\"得分(score)\", color='navy') plt.barh(indices + .3, training_time, .2, label=\"训练时间\", color='c') plt.barh(indices + .6, test_time, .2, label=\"最佳时间\", color='darkorange') plt.yticks(()) plt.legend(loc='best') plt.subplots_adjust(left=.25) plt.subplots_adjust(top=.95) plt.subplots_adjust(bottom=.05) for i, c in zip(indices, clf_names): plt.text(-.3, i, classifier_dic[c]) plt.show() 我们一直在努力 apachecn/sklearn-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-5'); const gitalk = new Gitalk({ clientID: 'c56c649fa375b552e607', clientSecret: '21401635722aa1b9518d38d5ff1f0f151b4786ca', repo: 'sklearn-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-07-03 15:34:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}